# Deadwood Forecasting Model

A multimodal transformer-based deep learning system for predicting deadwood occurrence and forest characteristics from satellite and climate data. The model integrates satellite imagery, elevation, canopy density, ERA5 climate data, and other geospatial features to forecast deadwood and forest dynamics.

This repository is currently in alpha stage. If you have any questions feel free to reach out.

## Table of Contents

- [Overview](#overview)
- [Features](#features)
- [Quick Start](#quick-start)
- [Installation](#installation)
- [Project Structure](#project-structure)
- [Data Pipeline](#data-pipeline)

## Overview

This project implements a **MultimodalDeadwoodTransformer** that predicts:
- **Annual Fractional Deadwood Cover Increase**

The model leverages:
- **Satellite imagery**: Sentinel-2 and other remote sensing data
- **Climate data**: ERA5 reanalysis (temperature, precipitation, etc.)
- **Geospatial features**: Terrain, canopy density, forest age
- **Transformer architecture**: Multi-head attention for capturing spatial-temporal dependencies

## Features

- ğŸŒ **Multimodal input**: Combines satellite, climate, and geospatial data
- ğŸ¤– **Transformer-based**: Multi-head attention architecture for improved feature interaction
- ğŸ“Š **Curriculum learning**: Rare sample infusion for better minority class performance
- ğŸ’¾ **Checkpoint management**: Automatic best model selection based on composite metrics
- ğŸ“ˆ **Comprehensive logging**: Timestamps, training curves, validation metrics
- ğŸ”§ **Flexible configuration**: YAML-based experiment management
- âš¡ **Distributed training**: Support for multiple GPUs and fold-based cross-validation
- ğŸ¯ **Multiple loss functions**: MSE, Huber, weighted variants

## Installation

### Prerequisites
- Python 3.12+
- CUDA 12.4+ (for GPU training)
- 100+ GB disk space (for data cubes)

### Setup Environment

```bash
# Create conda environment
conda create -n deadwood_forecasting_model python=3.12 -c conda-forge -y
conda activate deadwood_forecasting_model

# Install dependencies
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124
conda install xarray zarr zstandard rasterio rioxarray geopandas -y
conda install h5netcdf ipykernel s3fs pyarrow scikit-learn dask -y
conda install tqdm captum scikit-image -y
```

Or use the environment file:
```bash
conda env create -f docs/environment.yml
conda activate deadwood_forecasting_model
```

## Project Structure

```
deadwood_forecasting_model/
â”œâ”€â”€ analysis/                    # Data analysis and exploration
â”‚   â”œâ”€â”€ analysis.py
â”‚   â”œâ”€â”€ classification.py
â”‚   â”œâ”€â”€ ig_cube.py              # Integrated gradients analysis
â”‚   â”œâ”€â”€ model_selection.py
â”‚   â””â”€â”€ regression.py
â”‚
â”œâ”€â”€ config/                      # Configuration management
â”‚   â”œâ”€â”€ base.yaml               # Default settings
â”‚   â”œâ”€â”€ paths.py                # Centralized path definitions
â”‚   â””â”€â”€ experiments/            # Experiment-specific configs
â”‚       â”œâ”€â”€ run_00_dw_inc_f_tresh.yaml
â”‚       â”œâ”€â”€ run_01_dw_inc_f_tresh.yaml
â”‚       â”œâ”€â”€ run_02.yaml
â”‚       â””â”€â”€ ...
â”‚
â”œâ”€â”€ data/                        # Data storage
â”‚   â”œâ”€â”€ cubes/                  # Zarr format data cubes (0.zarr - 299.zarr)
â”‚   â”œâ”€â”€ figs/                   # Generated visualizations
â”‚   â”œâ”€â”€ logs/                   # Training logs
â”‚   â”œâ”€â”€ meta_data/              # Metadata tables
â”‚   â”œâ”€â”€ predictions/            # Model predictions
â”‚   â”œâ”€â”€ training_sets/          # Training/validation splits
â”‚   â””â”€â”€ training_runs/          # Experiment checkpoints & logs
â”‚
â”œâ”€â”€ docs/                        # Documentation
â”‚   â”œâ”€â”€ EXPERIMENT_GUIDE.md     # Detailed experiment workflow
â”‚   â”œâ”€â”€ env_setup.txt
â”‚   â””â”€â”€ environment.yml
â”‚
â”œâ”€â”€ inference/                   # Prediction pipeline
â”‚   â”œâ”€â”€ evaluate_model.py       # Model evaluation
â”‚   â”œâ”€â”€ predict_all_holdout_cubes.py
â”‚   â”œâ”€â”€ step07_create_specifc_cubes.py
â”‚   â”œâ”€â”€ step08_create_prediction.py
â”‚   â””â”€â”€ step09_analyze_predictions.py
â”‚
â”œâ”€â”€ models/                      # Neural network architectures
â”‚   â”œâ”€â”€ model.py                # v1: Single-output transformer
â”‚   â”œâ”€â”€ model2.py               # v2: Dual-output transformer
â”‚   â””â”€â”€ model_small.py          # Lightweight variant
â”‚
â”œâ”€â”€ processing/                  # Data preparation pipeline
â”‚   â”œâ”€â”€ step01_select_cubes.py
â”‚   â”œâ”€â”€ step02_built_cubes.py
â”‚   â”œâ”€â”€ step03_calculate_znorm_stats.py
â”‚   â”œâ”€â”€ step04_create_training_meta_table.py
â”‚   â”œâ”€â”€ step05_create_holdouts_folds.py
â”‚   â””â”€â”€ step06_create_trainingsset.py
â”‚
â”œâ”€â”€ training/                    # Training pipeline
â”‚   â”œâ”€â”€ trainer.py              # Main training entry point
â”‚   â”œâ”€â”€ train_utils.py          # Training utilities & config management
â”‚   â”œâ”€â”€ setup_training.py       # Dataset/dataloader setup
â”‚   â”œâ”€â”€ losses.py               # Custom loss functions
â”‚   â””â”€â”€ trainer.py
â”‚
â”œâ”€â”€ utils/                       # Utility functions
â”‚   â”œâ”€â”€ built_era5_cube.py      # ERA5 data processing
â”‚   â”œâ”€â”€ era5_downloader.py      # ERA5 download manager
â”‚   â”œâ”€â”€ means_dw_sentle.py      # Feature computation
â”‚   â”œâ”€â”€ parall.py               # Parallelization utilities
â”‚   â”œâ”€â”€ plots.py                # Visualization helpers
â”‚   â””â”€â”€ random.py
â”‚
â”œâ”€â”€ scripts/                     # Standalone analysis scripts
â”‚   â”œâ”€â”€ analyze_meta_table.py
â”‚   â”œâ”€â”€ analyze_training_set.py
â”‚   â””â”€â”€ select_locations.py
â”‚
â”œâ”€â”€ results/                     # Final results and outputs
â”‚   â””â”€â”€ full_scale_predictions/
â”‚
â””â”€â”€ README.md                    # This file
```

## Data Pipeline

### Data Flow

```
Raw Data (Sentinel-2, ERA5, DEM, etc.)
    â†“
step01_select_cubes.py      â†’ Select geographic regions
    â†“
step02_built_cubes.py       â†’ Create spatiotemporal Zarr cubes
    â†“
step03_calculate_znorm_stats.py â†’ Compute normalization statistics
    â†“
step04_create_training_meta_table.py â†’ Build metadata index
    â†“
step05_create_holdouts_folds.py â†’ Split into train/val folds
    â†“
step06_create_trainingsset.py â†’ Create training datasets
    â†“
Training / Inference
```

### Data Format

- **Cubes**: Zarr-formatted geospatial data cubes (3D: time Ã— height Ã— width)
  - Located in `data/cubes/{cube_id}.zarr/`
  - Each cube contains multiple spatiotemporal variables
  
- **Features**:
  - `deadwood_forest`: Deadwood observations
  - `terrain`: Elevation/slope
  - `canopy`: Canopy density
  - `pixels_sentle`: Sentinel-2 imagery
  - `era5`: Climate variables
  - `wc`: Worldclim data
  - `sg`: Soil/geology
  - `stand_age`: Forest age
